{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import load_WRDS\n",
    "import load_assets\n",
    "import config\n",
    "import Clean_data\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "import time\n",
    "#watch out\n",
    "#prereorganizingtotidydata\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import yfinance as yf\n",
    "\n",
    "DATA_DIR = config.DATA_DIR\n",
    "OUTPUT_DIR = config.OUTPUT_DIR\n",
    "\n",
    "# Load the data from WRDS\n",
    "rcfd_series_1 = load_WRDS.load_RCFD_series_1(data_dir=DATA_DIR)\n",
    "rcon_series_1 = load_WRDS.load_RCON_series_1(data_dir=DATA_DIR)\n",
    "rcfd_series_2 = load_WRDS.load_RCFD_series_2(data_dir=DATA_DIR)\n",
    "rcon_series_2 = load_WRDS.load_RCON_series_2(data_dir=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookie consent not found or already dismissed: Message: element click intercepted: Element is not clickable at point (885, 861)\n",
      "  (Session info: chrome=134.0.6998.89)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000101469804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000101461ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x0000000100fb5ea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x0000000101002c6c cxxbridge1$string$len + 407748\n",
      "4   chromedriver                        0x00000001010011d8 cxxbridge1$string$len + 400944\n",
      "5   chromedriver                        0x0000000100ffefec cxxbridge1$string$len + 392260\n",
      "6   chromedriver                        0x0000000100ffe3e8 cxxbridge1$string$len + 389184\n",
      "7   chromedriver                        0x0000000100ff2dec cxxbridge1$string$len + 342596\n",
      "8   chromedriver                        0x0000000100ff2878 cxxbridge1$string$len + 341200\n",
      "9   chromedriver                        0x000000010103e678 cxxbridge1$string$len + 651984\n",
      "10  chromedriver                        0x0000000100ff135c cxxbridge1$string$len + 335796\n",
      "11  chromedriver                        0x000000010142ecd4 cxxbridge1$str$ptr + 2545532\n",
      "12  chromedriver                        0x0000000101431fa0 cxxbridge1$str$ptr + 2558536\n",
      "13  chromedriver                        0x000000010140ed04 cxxbridge1$str$ptr + 2414508\n",
      "14  chromedriver                        0x0000000101432800 cxxbridge1$str$ptr + 2560680\n",
      "15  chromedriver                        0x00000001013ffba0 cxxbridge1$str$ptr + 2352712\n",
      "16  chromedriver                        0x000000010145245c cxxbridge1$str$ptr + 2690820\n",
      "17  chromedriver                        0x00000001014525e4 cxxbridge1$str$ptr + 2691212\n",
      "18  chromedriver                        0x0000000101461a50 cxxbridge1$str$ptr + 2753784\n",
      "19  libsystem_pthread.dylib             0x0000000188276f94 _pthread_start + 136\n",
      "20  libsystem_pthread.dylib             0x0000000188271d34 thread_start + 8\n",
      "\n",
      "Clicked the '10 Year' button.\n",
      "Clicked the 'Export' button.\n",
      "sleep time\n",
      "Cookie consent not found or already dismissed: Message: element click intercepted: Element is not clickable at point (885, 868)\n",
      "  (Session info: chrome=134.0.6998.89)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102b49804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000102b41ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x0000000102695ea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x00000001026e2c6c cxxbridge1$string$len + 407748\n",
      "4   chromedriver                        0x00000001026e11d8 cxxbridge1$string$len + 400944\n",
      "5   chromedriver                        0x00000001026defec cxxbridge1$string$len + 392260\n",
      "6   chromedriver                        0x00000001026de3e8 cxxbridge1$string$len + 389184\n",
      "7   chromedriver                        0x00000001026d2dec cxxbridge1$string$len + 342596\n",
      "8   chromedriver                        0x00000001026d2878 cxxbridge1$string$len + 341200\n",
      "9   chromedriver                        0x000000010271e678 cxxbridge1$string$len + 651984\n",
      "10  chromedriver                        0x00000001026d135c cxxbridge1$string$len + 335796\n",
      "11  chromedriver                        0x0000000102b0ecd4 cxxbridge1$str$ptr + 2545532\n",
      "12  chromedriver                        0x0000000102b11fa0 cxxbridge1$str$ptr + 2558536\n",
      "13  chromedriver                        0x0000000102aeed04 cxxbridge1$str$ptr + 2414508\n",
      "14  chromedriver                        0x0000000102b12800 cxxbridge1$str$ptr + 2560680\n",
      "15  chromedriver                        0x0000000102adfba0 cxxbridge1$str$ptr + 2352712\n",
      "16  chromedriver                        0x0000000102b3245c cxxbridge1$str$ptr + 2690820\n",
      "17  chromedriver                        0x0000000102b325e4 cxxbridge1$str$ptr + 2691212\n",
      "18  chromedriver                        0x0000000102b41a50 cxxbridge1$str$ptr + 2753784\n",
      "19  libsystem_pthread.dylib             0x0000000188276f94 _pthread_start + 136\n",
      "20  libsystem_pthread.dylib             0x0000000188271d34 thread_start + 8\n",
      "\n",
      "Clicked the '10 Year' button.\n",
      "Clicked the 'Export' button.\n",
      "1-3 downloaded\n",
      "Cookie consent not found or already dismissed: Message: element click intercepted: Element is not clickable at point (885, 868)\n",
      "  (Session info: chrome=134.0.6998.89)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000102921804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000102919ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x000000010246dea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x00000001024bac6c cxxbridge1$string$len + 407748\n",
      "4   chromedriver                        0x00000001024b91d8 cxxbridge1$string$len + 400944\n",
      "5   chromedriver                        0x00000001024b6fec cxxbridge1$string$len + 392260\n",
      "6   chromedriver                        0x00000001024b63e8 cxxbridge1$string$len + 389184\n",
      "7   chromedriver                        0x00000001024aadec cxxbridge1$string$len + 342596\n",
      "8   chromedriver                        0x00000001024aa878 cxxbridge1$string$len + 341200\n",
      "9   chromedriver                        0x00000001024f6678 cxxbridge1$string$len + 651984\n",
      "10  chromedriver                        0x00000001024a935c cxxbridge1$string$len + 335796\n",
      "11  chromedriver                        0x00000001028e6cd4 cxxbridge1$str$ptr + 2545532\n",
      "12  chromedriver                        0x00000001028e9fa0 cxxbridge1$str$ptr + 2558536\n",
      "13  chromedriver                        0x00000001028c6d04 cxxbridge1$str$ptr + 2414508\n",
      "14  chromedriver                        0x00000001028ea800 cxxbridge1$str$ptr + 2560680\n",
      "15  chromedriver                        0x00000001028b7ba0 cxxbridge1$str$ptr + 2352712\n",
      "16  chromedriver                        0x000000010290a45c cxxbridge1$str$ptr + 2690820\n",
      "17  chromedriver                        0x000000010290a5e4 cxxbridge1$str$ptr + 2691212\n",
      "18  chromedriver                        0x0000000102919a50 cxxbridge1$str$ptr + 2753784\n",
      "19  libsystem_pthread.dylib             0x0000000188276f94 _pthread_start + 136\n",
      "20  libsystem_pthread.dylib             0x0000000188271d34 thread_start + 8\n",
      "\n",
      "Clicked the '10 Year' button.\n",
      "Clicked the 'Export' button.\n",
      "3-5 downloaded\n",
      "Cookie consent not found or already dismissed: Message: element click intercepted: Element is not clickable at point (885, 864)\n",
      "  (Session info: chrome=134.0.6998.89)\n",
      "Stacktrace:\n",
      "0   chromedriver                        0x0000000103251804 cxxbridge1$str$ptr + 2785964\n",
      "1   chromedriver                        0x0000000103249ddc cxxbridge1$str$ptr + 2754692\n",
      "2   chromedriver                        0x0000000102d9dea8 cxxbridge1$string$len + 92928\n",
      "3   chromedriver                        0x0000000102deac6c cxxbridge1$string$len + 407748\n",
      "4   chromedriver                        0x0000000102de91d8 cxxbridge1$string$len + 400944\n",
      "5   chromedriver                        0x0000000102de6fec cxxbridge1$string$len + 392260\n",
      "6   chromedriver                        0x0000000102de63e8 cxxbridge1$string$len + 389184\n",
      "7   chromedriver                        0x0000000102ddadec cxxbridge1$string$len + 342596\n",
      "8   chromedriver                        0x0000000102dda878 cxxbridge1$string$len + 341200\n",
      "9   chromedriver                        0x0000000102e26678 cxxbridge1$string$len + 651984\n",
      "10  chromedriver                        0x0000000102dd935c cxxbridge1$string$len + 335796\n",
      "11  chromedriver                        0x0000000103216cd4 cxxbridge1$str$ptr + 2545532\n",
      "12  chromedriver                        0x0000000103219fa0 cxxbridge1$str$ptr + 2558536\n",
      "13  chromedriver                        0x00000001031f6d04 cxxbridge1$str$ptr + 2414508\n",
      "14  chromedriver                        0x000000010321a800 cxxbridge1$str$ptr + 2560680\n",
      "15  chromedriver                        0x00000001031e7ba0 cxxbridge1$str$ptr + 2352712\n",
      "16  chromedriver                        0x000000010323a45c cxxbridge1$str$ptr + 2690820\n",
      "17  chromedriver                        0x000000010323a5e4 cxxbridge1$str$ptr + 2691212\n",
      "18  chromedriver                        0x0000000103249a50 cxxbridge1$str$ptr + 2753784\n",
      "19  libsystem_pthread.dylib             0x0000000188276f94 _pthread_start + 136\n",
      "20  libsystem_pthread.dylib             0x0000000188271d34 thread_start + 8\n",
      "\n",
      "Clicked the '10 Year' button.\n",
      "Clicked the 'Export' button.\n",
      "7-10 downloaded\n"
     ]
    }
   ],
   "source": [
    "# Load S&P Treasury Index Data\n",
    "\n",
    "##################################################################\n",
    "###DATA PULL\n",
    "\n",
    "###################################################################\n",
    "\n",
    "try:\n",
    "    project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    project_dir = os.getcwd()\n",
    "download_dir = project_dir \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_dir}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.spglobal.com/spdji/en/indices/fixed-income/sp-us-treasury-bond-index/\")\n",
    "\n",
    "wait = WebDriverWait(driver, 30)\n",
    "\n",
    "try:\n",
    "    cookie_button = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "    cookie_button.click()\n",
    "    print(\"Cookie consent accepted.\")\n",
    "except Exception as e:\n",
    "    print(\"Cookie consent not found or already dismissed:\", e)\n",
    "\n",
    "time.sleep(2)  \n",
    "\n",
    "ten_year_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]/ul/li[7]/a'\n",
    "ten_year_button = wait.until(EC.element_to_be_clickable((By.XPATH, ten_year_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", ten_year_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", ten_year_button)\n",
    "print(\"Clicked the '10 Year' button.\")\n",
    "\n",
    "export_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[2]/div[1]/a'\n",
    "export_button = wait.until(EC.element_to_be_clickable((By.XPATH, export_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", export_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", export_button)\n",
    "print(\"Clicked the 'Export' button.\")\n",
    "\n",
    "downloaded_file = os.path.join(download_dir, \"data.xlsx\")\n",
    "\n",
    "print(\"sleep time\")\n",
    "time.sleep(5)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "#############################\n",
    "\n",
    "try:\n",
    "    project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    project_dir = os.getcwd()\n",
    "download_dir = project_dir \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_dir}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.spglobal.com/spdji/en/indices/fixed-income/sp-us-treasury-bond-1-3-year-index/#overview\")\n",
    "\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    cookie_button = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "    cookie_button.click()\n",
    "    print(\"Cookie consent accepted.\")\n",
    "except Exception as e:\n",
    "    print(\"Cookie consent not found or already dismissed:\", e)\n",
    "\n",
    "time.sleep(2)  \n",
    "ten_year_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]/ul/li[7]/a'\n",
    "ten_year_button = wait.until(EC.element_to_be_clickable((By.XPATH, ten_year_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", ten_year_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", ten_year_button)\n",
    "print(\"Clicked the '10 Year' button.\")\n",
    "\n",
    "export_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[2]/div[1]/a'\n",
    "export_button = wait.until(EC.element_to_be_clickable((By.XPATH, export_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", export_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", export_button)\n",
    "print(\"Clicked the 'Export' button.\")\n",
    "\n",
    "print(\"1-3 downloaded\")\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "#########################\n",
    "\n",
    "try:\n",
    "    project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    project_dir = os.getcwd()\n",
    "download_dir = project_dir \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_dir}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.spglobal.com/spdji/en/indices/fixed-income/sp-us-treasury-bond-3-5-year-index/#overview\")\n",
    "\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    cookie_button = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "    cookie_button.click()\n",
    "    print(\"Cookie consent accepted.\")\n",
    "except Exception as e:\n",
    "    print(\"Cookie consent not found or already dismissed:\", e)\n",
    "\n",
    "time.sleep(2)  \n",
    "ten_year_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]/ul/li[7]/a'\n",
    "ten_year_button = wait.until(EC.element_to_be_clickable((By.XPATH, ten_year_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", ten_year_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", ten_year_button)\n",
    "print(\"Clicked the '10 Year' button.\")\n",
    "\n",
    "export_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[2]/div[1]/a'\n",
    "export_button = wait.until(EC.element_to_be_clickable((By.XPATH, export_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", export_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", export_button)\n",
    "print(\"Clicked the 'Export' button.\")\n",
    "\n",
    "print(\"3-5 downloaded\")\n",
    "time.sleep(5)\n",
    "driver.quit()\n",
    "\n",
    "########################\n",
    "\n",
    "try:\n",
    "    project_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    project_dir = os.getcwd()\n",
    "download_dir = project_dir \n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_dir}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.spglobal.com/spdji/en/indices/fixed-income/sp-us-treasury-bond-7-10-year-index/#overview\")\n",
    "\n",
    "wait = WebDriverWait(driver, 15)\n",
    "\n",
    "try:\n",
    "    cookie_button = wait.until(EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\")))\n",
    "    cookie_button.click()\n",
    "    print(\"Cookie consent accepted.\")\n",
    "except Exception as e:\n",
    "    print(\"Cookie consent not found or already dismissed:\", e)\n",
    "\n",
    "time.sleep(2)  \n",
    "ten_year_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[1]/ul/li[7]/a'\n",
    "ten_year_button = wait.until(EC.element_to_be_clickable((By.XPATH, ten_year_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", ten_year_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", ten_year_button)\n",
    "print(\"Clicked the '10 Year' button.\")\n",
    "\n",
    "export_xpath = '//*[@id=\"overview-main\"]/div[1]/div[3]/div[2]/div[2]/div[1]/div[2]/div[2]/div[1]/a'\n",
    "export_button = wait.until(EC.element_to_be_clickable((By.XPATH, export_xpath)))\n",
    "driver.execute_script(\"arguments[0].scrollIntoView(true);\", export_button)\n",
    "time.sleep(1)\n",
    "driver.execute_script(\"arguments[0].click();\", export_button)\n",
    "print(\"Clicked the 'Export' button.\")\n",
    "\n",
    "print(\"7-10 downloaded\")\n",
    "time.sleep(5)\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "##################################################################\n",
    "###DATA CLEAN\n",
    "\n",
    "###################################################################\n",
    "\n",
    "#TRES INDEX CLEAN\n",
    "\n",
    "df = pd.read_excel('PerformanceGraphExport.xls')\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df = df.dropna(how='all')\n",
    "\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "num_cols = df.select_dtypes(include='number').columns\n",
    "df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "obj_cols = df.select_dtypes(include='object').columns\n",
    "df[obj_cols] = df[obj_cols].fillna('')\n",
    "\n",
    "df = df.iloc[2:, :]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df.rename(columns={df.columns[0]: \"date\", df.columns[1]: \"S&P U.S. Treasury Bond Index\"}, inplace=True)\n",
    "\n",
    "df = df.iloc[:-1, :]\n",
    "\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "\n",
    "start_date = pd.to_datetime(\"2021-02-26\")\n",
    "end_date = pd.to_datetime(\"2024-02-26\")\n",
    "\n",
    "df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_SP_Treasury_bond_index = df\n",
    "df_SP_Treasury_bond_index_1 = df_SP_Treasury_bond_index.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     date  sp 1-3\n",
      "0     2019-03-08 00:00:00   322.2\n",
      "1     2019-03-11 00:00:00  322.18\n",
      "2     2019-03-12 00:00:00  322.36\n",
      "3     2019-03-13 00:00:00  322.37\n",
      "4     2019-03-14 00:00:00  322.35\n",
      "...                   ...     ...\n",
      "1243  2024-02-26 00:00:00   340.9\n",
      "1244  2024-02-27 00:00:00  340.89\n",
      "1245  2024-02-28 00:00:00  341.25\n",
      "1246  2024-02-29 00:00:00  341.38\n",
      "1247  2024-03-01 00:00:00  342.03\n",
      "\n",
      "[1248 rows x 2 columns]\n",
      "                     date  sp 3-5\n",
      "0     2019-03-08 00:00:00  451.01\n",
      "1     2019-03-11 00:00:00  450.87\n",
      "2     2019-03-12 00:00:00  451.44\n",
      "3     2019-03-13 00:00:00   451.4\n",
      "4     2019-03-14 00:00:00  451.23\n",
      "...                   ...     ...\n",
      "1243  2024-02-26 00:00:00  465.72\n",
      "1244  2024-02-27 00:00:00  465.45\n",
      "1245  2024-02-28 00:00:00  466.46\n",
      "1246  2024-02-29 00:00:00  466.83\n",
      "1247  2024-03-01 00:00:00  468.52\n",
      "\n",
      "[1248 rows x 2 columns]\n",
      "                     date sp 7-10\n",
      "0     2019-03-08 00:00:00  601.05\n",
      "1     2019-03-11 00:00:00  600.54\n",
      "2     2019-03-12 00:00:00  602.21\n",
      "3     2019-03-13 00:00:00  602.09\n",
      "4     2019-03-14 00:00:00  601.28\n",
      "...                   ...     ...\n",
      "1243  2024-02-26 00:00:00  591.18\n",
      "1244  2024-02-27 00:00:00  590.01\n",
      "1245  2024-02-28 00:00:00  592.02\n",
      "1246  2024-02-29 00:00:00  593.04\n",
      "1247  2024-03-01 00:00:00  595.88\n",
      "\n",
      "[1248 rows x 2 columns]\n",
      "YF.download() has changed argument auto_adjust default to True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load and clean the first dataset\n",
    "df13 = pd.read_excel('PerformanceGraphExport (1).xls')\n",
    "\n",
    "df13 = df13.drop_duplicates()\n",
    "df13 = df13.dropna(how='all')\n",
    "\n",
    "df13.columns = df13.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "num_cols = df13.select_dtypes(include='number').columns\n",
    "df13[num_cols] = df13[num_cols].fillna(0)\n",
    "\n",
    "obj_cols = df13.select_dtypes(include='object').columns\n",
    "df13[obj_cols] = df13[obj_cols].fillna('')\n",
    "\n",
    "df13 = df13.iloc[2:, :]\n",
    "\n",
    "df13.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df13.rename(columns={df13.columns[0]: \"date\"}, inplace=True)\n",
    "\n",
    "df13 = df13.iloc[:-1, :]\n",
    "\n",
    "df13.rename(columns={df13.columns[0]: \"date\", df13.columns[1]: \"sp 1-3\"}, inplace=True) \n",
    "\n",
    "start_date = pd.to_datetime(\"2019-03-08\")\n",
    "end_date = pd.to_datetime(\"2024-03-01\")\n",
    "\n",
    "df13 = df13[(df13['date'] >= start_date) & (df13['date'] <= end_date)]\n",
    "\n",
    "df13.reset_index(drop=True, inplace=True)\n",
    "\n",
    "##################################################################################\n",
    "#COMBINED SP35 CLEAN\n",
    "\n",
    "df35 = pd.read_excel('PerformanceGraphExport (2).xls')\n",
    "\n",
    "df35 = df35.drop_duplicates()\n",
    "df35 = df35.dropna(how='all')\n",
    "\n",
    "df35.columns = df35.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "num_cols = df35.select_dtypes(include='number').columns\n",
    "df35[num_cols] = df35[num_cols].fillna(0)\n",
    "\n",
    "obj_cols = df35.select_dtypes(include='object').columns\n",
    "df35[obj_cols] = df35[obj_cols].fillna('')\n",
    "\n",
    "df35 = df35.iloc[2:, :]\n",
    "\n",
    "df35.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df35.rename(columns={df35.columns[0]: \"date\"}, inplace=True)\n",
    "\n",
    "df35 = df35.iloc[:-1, :]\n",
    "\n",
    "df35.rename(columns={df35.columns[0]: \"date\", df35.columns[1]: \"sp 3-5\"}, inplace=True) \n",
    "\n",
    "start_date = pd.to_datetime(\"2019-03-08\")\n",
    "end_date = pd.to_datetime(\"2024-03-01\")\n",
    "\n",
    "df35 = df35[(df35['date'] >= start_date) & (df35['date'] <= end_date)]\n",
    "\n",
    "df35.reset_index(drop=True, inplace=True)\n",
    "\n",
    "###########################################################\n",
    "#COMBINED SP710 CLEAN\n",
    "\n",
    "df710 = pd.read_excel('PerformanceGraphExport (3).xls')\n",
    "\n",
    "df710 = df710.drop_duplicates()\n",
    "df710 = df710.dropna(how='all')\n",
    "\n",
    "df710.columns = df710.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "\n",
    "num_cols = df710.select_dtypes(include='number').columns\n",
    "df710[num_cols] = df710[num_cols].fillna(0)\n",
    "\n",
    "obj_cols = df710.select_dtypes(include='object').columns\n",
    "df710[obj_cols] = df710[obj_cols].fillna('')\n",
    "\n",
    "df710 = df710.iloc[2:, :]\n",
    "\n",
    "df710.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df710.rename(columns={df710.columns[0]: \"date\"}, inplace=True)\n",
    "\n",
    "df710 = df710.iloc[:-1, :]\n",
    "\n",
    "df710.rename(columns={df710.columns[0]: \"date\", df710.columns[1]: \"sp 7-10\"}, inplace=True) \n",
    "\n",
    "start_date = pd.to_datetime(\"2019-03-08\")\n",
    "end_date = pd.to_datetime(\"2024-03-01\")\n",
    "\n",
    "df710 = df710[(df710['date'] >= start_date) & (df710['date'] <= end_date)]\n",
    "\n",
    "df710.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df13)\n",
    "print(df35)\n",
    "print(df710)\n",
    "\n",
    "###############################################################################################\n",
    "#YFINANCE iShares\n",
    "tickers = {\n",
    "    \"iShares 0-1\": \"IB01.L\",   # iShares Short Treasury Bond ETF\n",
    "    \"iShares 1-3\": \"SHY\",   # iShares 1-3 Year Treasury Bond ETF\n",
    "    \"iShares 7-10\": \"IEF\",  # iShares 7-10 Year Treasury Bond ETF\n",
    "    \"iShares 10-20\": \"TLH\", # iShares 10-20 Year Treasury Bond ETF\n",
    "    \"iShares 20+\": \"TLT\"    # iShares 20+ Year Treasury Bond ETF\n",
    "}\n",
    "\n",
    "# Define date range\n",
    "start_date = \"2019-03-08\"\n",
    "end_date = \"2024-03-01\"\n",
    "\n",
    "# Fetch data\n",
    "iShares = yf.download(list(tickers.values()), start=start_date, end=end_date, progress=False)[\"Close\"]\n",
    "\n",
    "# Rename columns to match requested format\n",
    "iShares.columns = list(tickers.keys())\n",
    "\n",
    "# Reset index and rename date column\n",
    "iShares.reset_index(inplace=True)\n",
    "iShares.rename(columns={\"Date\": \"date\"}, inplace=True)\n",
    "\n",
    "\n",
    "################################################################################################\n",
    "#JOINING ISHARES AND COMBINED PT 1,2,3\n",
    "\n",
    "# Convert the 'date' column in all DataFrames to datetime format\n",
    "dfs = [iShares, df13, df35, df710]\n",
    "for df in dfs:\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Merge all DataFrames on the 'date' column using an outer join\n",
    "treasury_prices = iShares.merge(df13, on='date', how='outer') \\\n",
    "                   .merge(df35, on='date', how='outer') \\\n",
    "                   .merge(df710, on='date', how='outer')\n",
    "\n",
    "treasury_prices = treasury_prices.set_index(\"date\").asfreq(\"D\").ffill()\n",
    "treasury_prices = treasury_prices.reset_index()\n",
    "\n",
    "treasury_prices_1 = treasury_prices.copy()\n",
    "\n",
    "treasury_prices_1_all_dates = treasury_prices_1.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "report_losses() missing 1 required positional argument: 'treasury_prices_all_dates'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 436\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[33;03mThe following code runs the statistics for table 1 (as in the paper)\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m    432\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    434\u001b[39m \u001b[38;5;66;03m##Calculations for all banks##################################################################################################################################### \u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# Calculate the losses \u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m bank_losses_assets = \u001b[43mreport_losses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_RMBS_Final\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_loans_first_lien_domestic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_treasury_and_others\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_other_loan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtreasury_prices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRMBS_multiplier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_asset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;66;03m# Calculate the uninsured deposit/MM asset ratio\u001b[39;00m\n\u001b[32m    439\u001b[39m uninsured_deposit_mm_asset = calculate_uninsured_deposit_mm_asset(uninsured_deposit, bank_losses_assets)\n",
      "\u001b[31mTypeError\u001b[39m: report_losses() missing 1 required positional argument: 'treasury_prices_all_dates'"
     ]
    }
   ],
   "source": [
    "# Conduct the calculations\n",
    "\n",
    "###################################################################################\n",
    "#YFINANCE MBS automation\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "# Define the ticker symbol\n",
    "ticker_symbol = \"MBB\"\n",
    "\n",
    "# Set the date range based on your dataset\n",
    "start_date = \"2019-03-11\"\n",
    "end_date = \"2024-03-08\"\n",
    "\n",
    "# Fetch historical market data for MBB within the same date range\n",
    "df_iShare_MBS_ETF = yf.download(ticker_symbol, start=start_date, end=end_date)\n",
    "df_iShare_MBS_ETF.columns = df_iShare_MBS_ETF.columns.get_level_values(0)\n",
    "df_iShare_MBS_ETF.rename(columns={\"Close\": \"Adj Close\"}, inplace=True)\n",
    "df_iShare_MBS_ETF = df_iShare_MBS_ETF.reset_index()\n",
    "df_iShare_MBS_ETF_1 = df_iShare_MBS_ETF.copy()\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "# Load the data from the manual\n",
    "def RMBs_Multiplier(df_SP_Treasury_bond_index, df_iShare_MBS_ETF, start_date = '2022-03-31', end_date = '2023-03-31'):\n",
    "    \"\"\"\n",
    "    Calculate the multiplier for the MBS assets based on the change in the S&P U.S. Treasury Bond Index and iShares MBS ETF.\n",
    "\n",
    "    Parameters:\n",
    "    df_SP_Treasury_bond_index (pd.DataFrame): DataFrame containing the S&P U.S. Treasury Bond Index data.\n",
    "    df_iShare_MBS_ETF (pd.DataFrame): DataFrame containing the iShares MBS ETF data.\n",
    "    start_date (str): Start date for the calculation (default is '2022-03-31').\n",
    "\n",
    "    Returns:\n",
    "    float: The multiplier for the MBS assets based on the change in the S&P U.S. Treasury Bond Index and iShares MBS ETF.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    upper_treasury = df_SP_Treasury_bond_index.loc[end_date, 'S&P U.S. Treasury Bond Index']\n",
    "    lower_treasury = df_SP_Treasury_bond_index.loc[start_date, 'S&P U.S. Treasury Bond Index']\n",
    "    \n",
    "    upper_MBS = df_iShare_MBS_ETF.loc[end_date, 'Adj Close']\n",
    "    lower_MBS = df_iShare_MBS_ETF.loc[start_date, 'Adj Close']\n",
    "    \n",
    "    MBS_change = (upper_MBS / lower_MBS) - 1\n",
    "    treasury_change = (upper_treasury / lower_treasury) - 1\n",
    "    multiplier = MBS_change / treasury_change\n",
    "    \n",
    "    return multiplier\n",
    "\n",
    "def report_losses(df_RMBS_Final, df_loans_first_lien_domestic, df_treasury_and_others, df_other_loan, treasury_prices, RMBS_multiplier, df_asset, start_date = '2022-03-31', end_date = '2023-03-31'):      \n",
    "    \"\"\"\n",
    "    Calculate the losses for each asset type based on the change in the market indices.\n",
    "\n",
    "    Parameters:\n",
    "    df_RMBS_Final (pd.DataFrame): DataFrame containing the RMBS assets data.\n",
    "    df_loans_first_lien_domestic (pd.DataFrame): DataFrame containing the loans data.\n",
    "    df_treasury_and_others (pd.DataFrame): DataFrame containing the treasury and other assets data.\n",
    "    df_other_loan (pd.DataFrame): DataFrame containing the other loan assets data.\n",
    "    treasury_prices (pd.DataFrame): DataFrame containing the treasury prices data.\n",
    "    RMBS_multiplier (float): The multiplier for the MBS assets based on the change in the S&P U.S. Treasury Bond Index and iShares MBS ETF.\n",
    "    df_asset (pd.DataFrame): DataFrame containing the total assets data.\n",
    "    start_date (str): Start date for the calculation (default is '2022-03-31').\n",
    "    end_date (str): End date for the calculation (default is '2023-03-31').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the losses and assets for each bank.\n",
    "    \"\"\"  \n",
    "    # Calculate the price change for each treasury bond\n",
    "    price_change = {\n",
    "        '<1y': -0.02,\n",
    "        '1y-3y': -0.06,\n",
    "        '3y-5y': -0.10,\n",
    "        '7y-10y': -0.20,\n",
    "        '>20y': -0.30\n",
    "    }\n",
    "\n",
    "    # Define the mapping of buckets to be used for aggregation\n",
    "    bucket_mapping = {\n",
    "        '<3m': '<1y',\n",
    "        '3m-1y': '<1y',\n",
    "        '1y-3y': '1y-3y',\n",
    "        '3y-5y': '3y-5y',\n",
    "        '5y-15y': '7y-10y',  # Assuming '5y-15y' should be mapped to '7y-10y' based on provided price_change calculation\n",
    "        '>15y': '>20y',\n",
    "    }\n",
    "      \n",
    "    # Aggregate the assets for each bank\n",
    "    aggregated_assets = {}\n",
    "    for name, df in zip(['RMBS', 'Loans', 'Treasury', 'OtherLoan'], \n",
    "                        [df_RMBS_Final, df_loans_first_lien_domestic, df_treasury_and_others, df_other_loan]):\n",
    "        # Ensure columns for aggregation are present\n",
    "        columns_to_aggregate = [col for col in list(bucket_mapping.keys()) if col in df.columns]\n",
    "        aggregated_assets[name] = df.groupby(['bank_name', 'Bank_ID'])[columns_to_aggregate].sum().reset_index()\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    bank_losses_assets = pd.DataFrame(columns=[\n",
    "        'bank_name', 'bank_ID', 'RMBs_loss', 'treasury_loss', 'loans_loss', 'other_loan_loss', \n",
    "        'total_loss', 'Share RMBs', 'Share Treasury and Other', \n",
    "        'Share Residential Mortgage', 'Share Other Loan', 'RMBs_asset', 'treasury_asset', \n",
    "        'residential_mortgage_asset', 'other_loan_asset', 'core_asset', 'gross_asset', 'loss/core_asset', 'loss/gross_asset',\n",
    "    ])\n",
    "    \n",
    "    # Iterate over each bank to calculate losses and assets\n",
    "    for _, df_row in df_asset.iterrows():\n",
    "        bank = df_row['bank_name']\n",
    "        bank_id = df_row['Bank_ID']\n",
    "        bank_total_asset = df_row['gross_asset']\n",
    "        \n",
    "        #Initialize variables for loss and asset calculations\n",
    "        rmbs_loss = loans_loss = treasury_loss = other_loan_loss = total_loss = 0\n",
    "        rmbs_asset = treasury_asset = loan_asset = other_loan_asset = core_asset = 0\n",
    "        \n",
    "        #Calculating losses for RMBs\n",
    "        if 'RMBS' in aggregated_assets and not aggregated_assets['RMBS'].empty:\n",
    "            rmbs_row = aggregated_assets['RMBS'][(aggregated_assets['RMBS']['bank_name'] == bank) & (aggregated_assets['RMBS']['Bank_ID'] == bank_id)]\n",
    "            for bucket, treasury_bucket in bucket_mapping.items():\n",
    "                if bucket in rmbs_row.columns:\n",
    "                    asset_amount = rmbs_row.iloc[0][bucket] if not rmbs_row.empty else 0\n",
    "                    rmbs_loss += (asset_amount * RMBS_multiplier * price_change[treasury_bucket])\n",
    "                    rmbs_asset += asset_amount\n",
    "                    \n",
    "        #Calculating losses for loans\n",
    "        loans_row = aggregated_assets['Loans'][(aggregated_assets['Loans']['bank_name'] == bank) & (aggregated_assets['Loans']['Bank_ID'] == bank_id)]\n",
    "        if not loans_row.empty:\n",
    "            for bucket, treasury_bucket in bucket_mapping.items():\n",
    "                if bucket in loans_row.columns:\n",
    "                    asset_amount = loans_row.iloc[0][bucket]\n",
    "                    loans_loss += (asset_amount * RMBS_multiplier * price_change[treasury_bucket])\n",
    "                    loan_asset += asset_amount\n",
    "\n",
    "        #Calculating Treasuries\n",
    "        treasury_row = aggregated_assets['Treasury'][(aggregated_assets['Treasury']['bank_name'] == bank) & (aggregated_assets['Treasury']['Bank_ID'] == bank_id)]\n",
    "        if not treasury_row.empty:\n",
    "            for bucket, treasury_bucket in bucket_mapping.items():\n",
    "                if bucket in treasury_row.columns:\n",
    "                    asset_amount = treasury_row.iloc[0][bucket]\n",
    "                    treasury_loss += (asset_amount * price_change[treasury_bucket])\n",
    "                    treasury_asset += asset_amount\n",
    "\n",
    "        #Other loans\n",
    "        other_loan_row = aggregated_assets['OtherLoan'][(aggregated_assets['OtherLoan']['bank_name'] == bank) & (aggregated_assets['OtherLoan']['Bank_ID'] == bank_id)]\n",
    "        if not other_loan_row.empty:\n",
    "            for bucket, treasury_bucket in bucket_mapping.items():\n",
    "                if bucket in other_loan_row.columns:\n",
    "                    asset_amount = other_loan_row.iloc[0][bucket]\n",
    "                    other_loan_loss += (asset_amount * price_change[treasury_bucket])\n",
    "                    other_loan_asset += asset_amount\n",
    "\n",
    "        # Calculate total loss and core asset      \n",
    "        total_loss = rmbs_loss + treasury_loss + loans_loss + other_loan_loss\n",
    "        core_asset = rmbs_asset + treasury_asset + loan_asset + other_loan_asset\n",
    "\n",
    "        # Append the results to the DataFrame\n",
    "        bank_losses_assets.loc[len(bank_losses_assets)] = {\n",
    "            'bank_name': bank,\n",
    "            'bank_ID': bank_id,\n",
    "            'RMBs_loss': rmbs_loss,\n",
    "            'treasury_loss': treasury_loss,\n",
    "            'loans_loss': loans_loss,\n",
    "            'other_loan_loss': other_loan_loss,\n",
    "            'total_loss': total_loss,\n",
    "            'Share RMBs': rmbs_loss / total_loss if total_loss else 0,\n",
    "            'Share Treasury and Other': treasury_loss / total_loss if total_loss else 0,\n",
    "            'Share Residential Mortgage': loans_loss / total_loss if total_loss else 0,\n",
    "            'Share Other Loan': other_loan_loss / total_loss if total_loss else 0,\n",
    "            'RMBs_asset': rmbs_asset,\n",
    "            'treasury_asset': treasury_asset,\n",
    "            'residential_mortgage_asset': loan_asset,\n",
    "            'other_loan_asset': other_loan_asset,\n",
    "            'core_asset': core_asset,\n",
    "            'gross_asset': bank_total_asset,\n",
    "            'loss/core_asset': -(total_loss / core_asset) if core_asset else 0,\n",
    "            'loss/gross_asset': -(total_loss / bank_total_asset) if bank_total_asset else 0,\n",
    "        }\n",
    "\n",
    "    return bank_losses_assets\n",
    "\n",
    "def calculate_uninsured_deposit_mm_asset(uninsured_deposit, bank_losses_assets):\n",
    "    \"\"\"\n",
    "    Calculate the uninsured deposit/MM asset ratio for each bank.\n",
    "\n",
    "    Parameters:\n",
    "    uninsured_deposit (pd.DataFrame): DataFrame containing the uninsured deposit data.\n",
    "    bank_losses_assets (pd.DataFrame): DataFrame containing the losses and assets for each bank.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the uninsured deposit/MM asset ratio for each bank.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Adjust the uninsured_deposit DataFrame to use both 'bank_name' and 'Bank_ID' as a multi-index for quick lookup\n",
    "    uninsured_lookup = uninsured_deposit.set_index(['bank_name', 'bank_ID'])['uninsured_deposit'].to_dict()\n",
    "    \n",
    "    # Iterate over each row in bank_losses DataFrame\n",
    "    for _, bank_loss_row in bank_losses_assets.iterrows():\n",
    "        bank_name = bank_loss_row['bank_name']\n",
    "        bank_id = bank_loss_row['bank_ID']\n",
    "        \n",
    "        # Adjust the lookup to include 'Bank_ID'\n",
    "        uninsured_deposit_value = uninsured_lookup.get((bank_name, bank_id), 0)\n",
    "        \n",
    "        # Calculate 'MM Asset' as (as defined in the paper)\n",
    "        mm_asset = bank_loss_row['total_loss'] + bank_loss_row['gross_asset']\n",
    "        \n",
    "        # Calculate Uninsured Deposit/MM Asset ratio \n",
    "        if mm_asset > 0:\n",
    "            uninsured_deposit_mm_asset_ratio = uninsured_deposit_value / mm_asset\n",
    "        \n",
    "        # Append to final dataframe\n",
    "        results.append({\n",
    "            'bank_name': bank_name,\n",
    "            'bank_ID': bank_id, \n",
    "            'total_loss': bank_loss_row['total_loss'], \n",
    "            'total_asset': bank_loss_row['gross_asset'],\n",
    "            'mm_asset': mm_asset,\n",
    "            'uninsured_deposit': uninsured_deposit_value, \n",
    "            'Uninsured_Deposit_MM_Asset': uninsured_deposit_mm_asset_ratio\n",
    "        })\n",
    "\n",
    "    # Convert results list to DataFrame and sort by 'Bank_ID'\n",
    "    uninsured_deposit_mm_asset = pd.DataFrame(results).sort_values(by=['bank_name', 'bank_ID'])\n",
    "    \n",
    "    return uninsured_deposit_mm_asset\n",
    "\n",
    "def insured_deposit_coverage_ratio(insured_deposit, uninsured_deposit, bank_losses):\n",
    "    \"\"\"\n",
    "    Calculate the insured deposit coverage ratio for each bank.\n",
    "\n",
    "    Parameters:\n",
    "    insured_deposit (pd.DataFrame): DataFrame containing the insured deposit data.\n",
    "    uninsured_deposit (pd.DataFrame): DataFrame containing the uninsured deposit data.\n",
    "    bank_losses (pd.DataFrame): DataFrame containing the losses and assets for each bank.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the insured deposit coverage ratio for each bank.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "    \n",
    "    # Create dictionaries from insured and uninsured deposits for quick lookup\n",
    "    insured_lookup = insured_deposit.set_index(['bank_name', 'bank_ID'])['insured_deposit'].to_dict()\n",
    "    uninsured_lookup = uninsured_deposit.set_index(['bank_name', 'bank_ID'])['uninsured_deposit'].to_dict()\n",
    "    \n",
    "    # Iterate over each row in bank_losses DataFrame\n",
    "    for _, bank_loss_row in bank_losses.iterrows():\n",
    "        bank_name = bank_loss_row['bank_name']\n",
    "        bank_id = bank_loss_row['bank_ID']\n",
    "        \n",
    "        # Retrieve insured and uninsured deposit values\n",
    "        insured_deposit_value = insured_lookup.get((bank_name, bank_id), 0)\n",
    "        uninsured_deposit_value = uninsured_lookup.get((bank_name, bank_id), 0)\n",
    "        \n",
    "        # Calculate mark-to-market asset value \n",
    "        mark_to_market_asset_value = bank_loss_row['total_loss'] + bank_loss_row['gross_asset']\n",
    "        \n",
    "        # Calculate the insured deposit coverage ratio\n",
    "        if insured_deposit_value > 0:  # Prevent division by zero\n",
    "            coverage_ratio = (mark_to_market_asset_value - uninsured_deposit_value - insured_deposit_value) / insured_deposit_value\n",
    "        \n",
    "        # Append the result\n",
    "        results.append({\n",
    "            'bank_name': bank_name,\n",
    "            'bank_ID': bank_id,\n",
    "            'mm_asset': mark_to_market_asset_value,\n",
    "            'insured_deposit': insured_deposit_value,\n",
    "            'uninsured_deposit': uninsured_deposit_value,\n",
    "            'insured_deposit_coverage_ratio': coverage_ratio\n",
    "        })\n",
    "    \n",
    "    # Convert results list to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "def final_statistic_table(bank_losses_assets, uninsured_deposit_mm_asset, insured_deposit_coverage, index_name = 'All Banks'):\n",
    "    \"\"\"\n",
    "    Calculate the final statistics table for the banks.\n",
    "\n",
    "    Parameters:\n",
    "    bank_losses_assets (pd.DataFrame): DataFrame containing the losses and assets for each bank.\n",
    "    uninsured_deposit_mm_asset (pd.DataFrame): DataFrame containing the uninsured deposit/MM asset ratio for each bank.\n",
    "    insured_deposit_coverage (pd.DataFrame): DataFrame containing the insured deposit coverage ratio for each bank.\n",
    "    index_name (str): Name of the index (default is 'All Banks').\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the final statistics table.\n",
    "    \"\"\"\n",
    "    \n",
    "    bank_count = len(bank_losses_assets.index)\n",
    "\n",
    "    final_stats = pd.DataFrame({\n",
    "        'Aggregate Loss': [f\"{-round(bank_losses_assets['total_loss'].sum() / 1e9, 1)}T\"],  # Convert to trillions\n",
    "        'Bank Level Loss': [f\"{-round(bank_losses_assets['total_loss'].median() / 1e3, 1)}M\"],  # Convert to millions\n",
    "        'Bank Level Loss Std': [f\"{round(bank_losses_assets['total_loss'].std() / 1e6, 2)}B\"],  # Std deviation for Bank Level Loss\n",
    "        'Share RMBS': [round(bank_losses_assets['Share RMBs'].median() * 100, 1)],  # Median percentage\n",
    "        'Share RMBS Std': [round(bank_losses_assets['Share RMBs'].std() * 100, 1)],  # Std deviation for Share RMBS\n",
    "        'Share Treasury and Other': [round(bank_losses_assets['Share Treasury and Other'].median() * 100, 1)],  # Median percentage\n",
    "        'Share Treasury and Other Std': [round(bank_losses_assets['Share Treasury and Other'].std() * 100, 1)],  # Std deviation\n",
    "        'Share Residential Mortgage': [round(bank_losses_assets['Share Residential Mortgage'].median() * 100, 1)],  # Median percentage\n",
    "        'Share Residential Mortgage Std': [round(bank_losses_assets['Share Residential Mortgage'].std() * 100, 1)],  # Std deviation\n",
    "        'Share Other Loan': [round(bank_losses_assets['Share Other Loan'].median() * 100, 1)],  # Median percentage\n",
    "        'Share Other Loan Std': [round(bank_losses_assets['Share Other Loan'].std() * 100, 1)],  # Std deviation\n",
    "        'Loss/Asset': [round(bank_losses_assets['loss/gross_asset'].median() * 100, 1)],  # Median percentage\n",
    "        'Loss/Asset Std': [round(bank_losses_assets['loss/gross_asset'].std() * 100, 1)],  # Std deviation\n",
    "        'Uninsured Deposit/MM Asset': [round(uninsured_deposit_mm_asset['Uninsured_Deposit_MM_Asset'].median() * 100, 1)],  # Median percentage\n",
    "        'Uninsured Deposit/MM Asset Std': [round(uninsured_deposit_mm_asset['Uninsured_Deposit_MM_Asset'].std() * 100, 1)],  # Std deviation\n",
    "        'Insured Deposit Coverage Ratio': [round(insured_deposit_coverage['insured_deposit_coverage_ratio'].median() * 100, 1)],  # Median percentage\n",
    "        'Insured Deposit Coverage Ratio Std': [round(insured_deposit_coverage['insured_deposit_coverage_ratio'].std() * 100, 1)],  # Std deviation\n",
    "        'Number of Banks': [len(bank_losses_assets.index.unique())]  # Count of unique banks\n",
    "    })\n",
    "\n",
    "    # Rename index to 'All Banks'\n",
    "    final_stats.index = [index_name]\n",
    "\n",
    "    final_stats = final_stats.T # Transpose the DataFrame\n",
    "    \n",
    "    return final_stats\n",
    "\n",
    "def GSIB_bank_id():\n",
    "    \"\"\"\n",
    "    Returns a list of GSIB bank IDs.\n",
    "    \"\"\"\n",
    "    #GSIB = [35301,93619,229913,398668,413208,451965,476810,480228,488318,\n",
    "     #497404,541101,651448,688079,722777,812164,852218,934329,1225761,\n",
    "     #1443266,1456501,2182786,2362458,2489805,2531991,3066025]\n",
    "    GSIB = [852218, 480228, 476810, 413208, #JP Morgan, Bank of America, Citigroup, HSBC\n",
    "      2980209, 2182786, 541101, 655839, 1015560, 229913,#Barclays, Goldman Sachs, BNY Mellon, CCB COMMUNITY BANK, ICBC, Mizuho\n",
    "       1456501, 722777, 35301, 925411, 497404, 3212149, #Morgan Stanley, Santander, State Street, Sumitomo Mitsui, TD Bank, UBS\n",
    "      451965] #wells fargo\n",
    "    return GSIB\n",
    "\n",
    "def large_ex_GSIB_bank_id(large):\n",
    "    \"\"\"\n",
    "    Returns a list of large non-GSIB bank IDs.\n",
    "    \"\"\"\n",
    "    bank_id_large_ex_GSIB = []\n",
    "    for bank_id in large['Bank_ID']:\n",
    "       bank_id_large_ex_GSIB.append(bank_id)\n",
    "    return bank_id_large_ex_GSIB\n",
    "\n",
    "def small_bank_id(small):\n",
    "    \"\"\"\n",
    "    Returns a list of small bank IDs.\n",
    "    \"\"\"\n",
    "    bank_id_small = []\n",
    "    for bank_id in small['Bank_ID']:\n",
    "       bank_id_small.append(bank_id)\n",
    "    return bank_id_small\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ##Clean the dataframes for table 1##################################################################################################################################### \n",
    "    treasury_prices = Clean_data.clean_treasury_prices(treasury_prices, start_date = '2022-03-31', end_date = '2023-03-31')\n",
    "    df_SP_Treasury_bond_index = Clean_data.clean_sp_treasury_bond_index(df_SP_Treasury_bond_index, start_date = '2022-03-31', end_date = '2023-03-31')\n",
    "    df_iShare_MBS_ETF = Clean_data.clean_iShare_MBS_ETF(df_iShare_MBS_ETF, start_date = '2022-03-31', end_date = '2023-03-31')\n",
    "    RMBS_multiplier = RMBs_Multiplier(df_SP_Treasury_bond_index, df_iShare_MBS_ETF, start_date = '2022-03-31', end_date = '2023-03-31') #MBS multiplier\n",
    "\n",
    "    ##Prepare the dataframes for table 2 (with most up-to-date market indices data)##################################################################################################################################### \n",
    "    treasury_prices_updated = Clean_data.clean_treasury_prices(treasury_prices_1, start_date = '2022-03-31', end_date = '2023-12-31')\n",
    "    df_SP_Treasury_bond_index_updated = Clean_data.clean_sp_treasury_bond_index(df_SP_Treasury_bond_index_1, start_date = '2022-03-31', end_date = '2023-12-31')\n",
    "    df_iShare_MBS_ETF_updated = Clean_data.clean_iShare_MBS_ETF(df_iShare_MBS_ETF_1, start_date = '2022-03-31', end_date = '2023-12-31')\n",
    "    RMBS_multiplier_updated = RMBs_Multiplier(df_SP_Treasury_bond_index_updated, df_iShare_MBS_ETF_updated, start_date = '2022-03-31', end_date = '2023-12-31') #MBS multiplier\n",
    "    \n",
    "    ##Get the required dataframes##################################################################################################################################### \n",
    "    df_RMBS_Final = Clean_data.get_RMBs(rcfd_series_1, rcon_series_1)\n",
    "    df_loans_first_lien_domestic = Clean_data.get_loans(rcon_series_1)\n",
    "    df_treasury_and_others = Clean_data.get_treasuries(rcfd_series_2, rcon_series_2)\n",
    "    df_other_loan = Clean_data.get_other_loan(rcon_series_2, rcfd_series_1)\n",
    "    df_asset = Clean_data.get_total_asset(rcfd_series_2, rcon_series_2)\n",
    "    uninsured_deposit = Clean_data.get_uninsured_deposits(rcon_series_1)\n",
    "    insured_deposits = Clean_data.get_insured_deposits(rcon_series_1)\n",
    "\n",
    "    ##Sort the dataframes#####################################################################################################################################\n",
    "    df_asset = df_asset #total assets all banks\n",
    "    #GSIB Banks\n",
    "    GSIB = GSIB_bank_id() #list of GSIB bank IDs\n",
    "    df_asset_GSIB = df_asset[df_asset['Bank_ID'].isin(GSIB)] #total assets all GSIB banks\n",
    "    #Large non-GSIB Banks\n",
    "    df_asset_large_ex_GSIB = df_asset[(~df_asset['Bank_ID'].isin(GSIB)) & (df_asset['gross_asset']>1384000)] #total assets all large non-GSIB banks\n",
    "    large_ex_GSIB = large_ex_GSIB_bank_id(df_asset_large_ex_GSIB) #list of large non-GSIB bank IDs\n",
    "    #Small Banks\n",
    "    df_asset_small = df_asset[(~df_asset['Bank_ID'].isin(GSIB)) & (df_asset['gross_asset']<=1384000)] #total asset all small banks \n",
    "    small = small_bank_id(df_asset_small) #list of small bank IDs\n",
    "\n",
    "    ##Prepare each asset type###################################################################################################################################\n",
    "    #RMBS\n",
    "    df_RMBS_Final = df_RMBS_Final #RMBS for all banks \n",
    "    df_RMBS_GSIB = df_RMBS_Final[df_RMBS_Final['Bank_ID'].isin(GSIB)] #RMBS for GSIB banks\n",
    "    df_RMBS_large_ex_GSIB = df_RMBS_Final[df_RMBS_Final['Bank_ID'].isin(large_ex_GSIB)] #RMBS for large non-GSIB banks\n",
    "    df_RMBS_small = df_RMBS_Final[df_RMBS_Final['Bank_ID'].isin(small)] #RMBS for small banks\n",
    "\n",
    "    #Loans First Lien Domestic\n",
    "\n",
    "    df_loans_first_lien_domestic = df_loans_first_lien_domestic # loans first lien domestic for all banks\n",
    "    df_loans_first_lien_domestic_GSIB = df_loans_first_lien_domestic[df_loans_first_lien_domestic['Bank_ID'].isin(GSIB)] # loans first lien domestic for all GSIB banks\n",
    "    df_loans_first_lien_domestic_large_ex_GSIB = df_loans_first_lien_domestic[df_loans_first_lien_domestic['Bank_ID'].isin(large_ex_GSIB)] # loans first lien domestic for all large non-GSIB banks\n",
    "    df_loans_first_lien_domestic_small = df_loans_first_lien_domestic[df_loans_first_lien_domestic['Bank_ID'].isin(small)]\n",
    "\n",
    "    #Treasury and Others\n",
    "\n",
    "    df_treasury_and_others = df_treasury_and_others #treasury and others all banks \n",
    "    df_treasury_and_others_GSIB = df_treasury_and_others[df_treasury_and_others['Bank_ID'].isin(GSIB)] #treasury and others GSIB banks\n",
    "    df_treasury_and_others_large_ex_GSIB = df_treasury_and_others[df_treasury_and_others['Bank_ID'].isin(large_ex_GSIB)] #treasury and others large non-GSIB baanks \n",
    "    df_treasury_and_others_small = df_treasury_and_others[df_treasury_and_others['Bank_ID'].isin(small)] #treasury and others small banks \n",
    "    \n",
    "    #Other Loan \n",
    "\n",
    "    df_other_loan = df_other_loan #other loans for all banks \n",
    "    df_other_loan_GSIB = df_other_loan[df_other_loan['Bank_ID'].isin(GSIB)] #other loans for all GSIB banks \n",
    "    df_other_loan_large_ex_GSIB = df_other_loan[df_other_loan['Bank_ID'].isin(large_ex_GSIB)] #other loans for all large non-GSIB banks\n",
    "    df_other_loan_small = df_other_loan[df_other_loan['Bank_ID'].isin(small)] #other oans for all small banks \n",
    "\n",
    "    #uninsured deposits\n",
    "    uninsured_deposit = uninsured_deposit #uninsured deposits for all banks\n",
    "    uninsured_deposit_GSIB = uninsured_deposit[uninsured_deposit['bank_ID'].isin(GSIB)] #uninsured deposits for GSIB banks\n",
    "    uninsured_deposit_large_ex_GSIB = uninsured_deposit[uninsured_deposit['bank_ID'].isin(large_ex_GSIB)] #uninsured deposits for large non-GSIB banks\n",
    "    uninsured_deposit_small = uninsured_deposit[uninsured_deposit['bank_ID'].isin(small)] #uninsured deposits for small banks\n",
    "\n",
    "    #insured deposits\n",
    "    insured_deposits = insured_deposits #insured deposits for all banks\n",
    "    insured_deposits_GSIB = insured_deposits[insured_deposits['bank_ID'].isin(GSIB)] #insured deposits for GSIB banks\n",
    "    insured_deposits_large_ex_GSIB = insured_deposits[insured_deposits['bank_ID'].isin(large_ex_GSIB)] #insured deposits for large non-GSIB banks\n",
    "    insured_deposits_small = insured_deposits[insured_deposits['bank_ID'].isin(small)] #insured deposits for small banks\n",
    "\n",
    "    \"\"\"\n",
    "    The following code runs the statistics for table 1 (as in the paper)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    ##Calculations for all banks##################################################################################################################################### \n",
    "    # Calculate the losses \n",
    "    bank_losses_assets = report_losses(df_RMBS_Final, df_loans_first_lien_domestic, df_treasury_and_others, df_other_loan, treasury_prices, RMBS_multiplier, df_asset)\n",
    "    \n",
    "    # Calculate the uninsured deposit/MM asset ratio\n",
    "    uninsured_deposit_mm_asset = calculate_uninsured_deposit_mm_asset(uninsured_deposit, bank_losses_assets)\n",
    "\n",
    "    # Calculate the insured deposit coverage ratio\n",
    "    insured_deposit_coverage = insured_deposit_coverage_ratio(insured_deposits, uninsured_deposit, bank_losses_assets)\n",
    "    \n",
    "    # Calculate the final statistics table\n",
    "    final_stats = final_statistic_table(bank_losses_assets, uninsured_deposit_mm_asset, insured_deposit_coverage)\n",
    "    \n",
    "    ##################################################################################################################################################################\n",
    "\n",
    "    ##Calculations for all GSIB banks################################################################################################################################\n",
    "    # Calculate the losses \n",
    "    bank_losses_assets_GSIB = report_losses(df_RMBS_GSIB, df_loans_first_lien_domestic_GSIB, df_treasury_and_others_GSIB, df_other_loan_GSIB, treasury_prices, RMBS_multiplier, df_asset_GSIB)\n",
    "    \n",
    "    # Calculate the uninsured deposit/MM asset ratio\n",
    "    uninsured_deposit_mm_asset_GSIB = calculate_uninsured_deposit_mm_asset(uninsured_deposit_GSIB, bank_losses_assets_GSIB)\n",
    "\n",
    "    # Calculate the insured deposit coverage ratio\n",
    "    insured_deposit_coverage_GSIB = insured_deposit_coverage_ratio(insured_deposits_GSIB, uninsured_deposit_GSIB, bank_losses_assets_GSIB)\n",
    "    \n",
    "    # Calculate the final statistics table\n",
    "    final_stats_GSIB = final_statistic_table(bank_losses_assets_GSIB, uninsured_deposit_mm_asset_GSIB, insured_deposit_coverage_GSIB, index_name = 'GSIB Banks')\n",
    "    ##################################################################################################################################################################\n",
    "\n",
    "    ##Calculations for all Large non-GSIB banks################################################################################################################################\n",
    "    # Calculate the losses \n",
    "    bank_losses_assets_large_ex_GSIB = report_losses(df_RMBS_large_ex_GSIB, df_loans_first_lien_domestic_large_ex_GSIB, df_treasury_and_others_large_ex_GSIB, df_other_loan_large_ex_GSIB, treasury_prices, RMBS_multiplier, df_asset_large_ex_GSIB)\n",
    "    \n",
    "    # Calculate the uninsured deposit/MM asset ratio\n",
    "    uninsured_deposit_mm_asset_large_ex_GSIB = calculate_uninsured_deposit_mm_asset(uninsured_deposit_large_ex_GSIB, bank_losses_assets_large_ex_GSIB)\n",
    "\n",
    "    # Calculate the insured deposit coverage ratio\n",
    "    insured_deposit_coverage_large_ex_GSIB = insured_deposit_coverage_ratio(insured_deposits_large_ex_GSIB, uninsured_deposit_large_ex_GSIB, bank_losses_assets_large_ex_GSIB)\n",
    "    \n",
    "    # Calculate the final statistics table\n",
    "    final_stats_large_ex_GSIB = final_statistic_table(bank_losses_assets_large_ex_GSIB, uninsured_deposit_mm_asset_large_ex_GSIB, insured_deposit_coverage_large_ex_GSIB, index_name = 'Large Ex GSIB Banks')\n",
    "    ##################################################################################################################################################################\n",
    "\n",
    "    ##Calculations for small banks################################################################################################################################\n",
    "    # Calculate the losses \n",
    "    bank_losses_assets_small = report_losses(df_RMBS_small, df_loans_first_lien_domestic_small, df_treasury_and_others_small, df_other_loan_small, treasury_prices, RMBS_multiplier, df_asset_small)\n",
    "    \n",
    "    # Calculate the uninsured deposit/MM asset ratio\n",
    "    uninsured_deposit_mm_asset_small = calculate_uninsured_deposit_mm_asset(uninsured_deposit_small, bank_losses_assets_small)\n",
    "\n",
    "    # Calculate the insured deposit coverage ratio\n",
    "    insured_deposit_coverage_small = insured_deposit_coverage_ratio(insured_deposits_small, uninsured_deposit_small, bank_losses_assets_small)\n",
    "    \n",
    "    # Calculate the final statistics table\n",
    "    final_stats_small = final_statistic_table(bank_losses_assets_small, uninsured_deposit_mm_asset_small, insured_deposit_coverage_small, index_name = 'Small Banks')\n",
    "    ##################################################################################################################################################################\n",
    "\n",
    "    table_1 = pd.concat([final_stats, final_stats_small, final_stats_large_ex_GSIB, final_stats_GSIB], axis=1)\n",
    "\n",
    "    # Sets format for printing to LaTeX\n",
    "    float_format_func = lambda x: '{:.1f}'.format(x)\n",
    "    latex_table_string = table_1.to_latex(float_format=float_format_func)\n",
    "    path = OUTPUT_DIR / f'Table1.tex'\n",
    "    with open(path, \"w\") as text_file:\n",
    "        text_file.write(latex_table_string)\n",
    "\n",
    "    print(table_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This script is designed to verify that key values in a DataFrame called table_1 closely match expected reference values. \n",
    "It begins by defining helper functions: one (parse_value) converts formatted numeric strings like \"2.2T\" or \"347.6M\" into their corresponding float values \n",
    "by interpreting suffixes (T for trillions, B for billions, and M for millions) and removing extraneous characters, while the other (approximate_equal) checks \n",
    "if two numbers are within a specified tolerance. The main component is a unittest TestCase class that takes table_1 \n",
    "and a dictionary of reference values as parameters and then runs tests on specific rows—such as \"Aggregate Loss\", \"Number of Banks\", \n",
    "and \"Share RMBS\"—to ensure the actual values fall within acceptable tolerances of the expected ones. A helper function \n",
    "builds a test suite from these tests, and the main block demonstrates how to execute the tests, \n",
    "thereby ensuring that the computed values in table_1 are sufficiently close to the original reference data without hard-coding the values globally.\n",
    "\"\"\"\n",
    "\n",
    "def parse_value(value_str):\n",
    "    \"\"\"\n",
    "    Convert a string like '2.2T', '347.6M', '99.7B', or '6.5' into a float.\n",
    "    Handles negatives and removes extra characters (like commas or parentheses).\n",
    "    \"\"\"\n",
    "    # Clean up the string\n",
    "    value_str = value_str.strip().replace(',', '')\n",
    "    value_str = value_str.strip('()')\n",
    "    \n",
    "    sign = 1\n",
    "    if value_str.startswith('-'):\n",
    "        sign = -1\n",
    "        value_str = value_str[1:]\n",
    "    \n",
    "    # Extract numeric part and unit\n",
    "    match = re.match(r'([\\d\\.]+)([TBM]*)', value_str)\n",
    "    if not match:\n",
    "        try:\n",
    "            return sign * float(value_str)\n",
    "        except ValueError:\n",
    "            raise ValueError(f\"Cannot parse value string: {value_str}\")\n",
    "    \n",
    "    num_part = float(match.group(1))\n",
    "    unit = match.group(2)\n",
    "    \n",
    "    multiplier = 1.0\n",
    "    if unit == 'T':\n",
    "        multiplier = 1e12\n",
    "    elif unit == 'B':\n",
    "        multiplier = 1e9\n",
    "    elif unit == 'M':\n",
    "        multiplier = 1e6\n",
    "        \n",
    "    return sign * num_part * multiplier\n",
    "\n",
    "def approximate_equal(actual, expected, tolerance):\n",
    "    \"\"\"Return True if actual is within ±tolerance of expected.\"\"\"\n",
    "    return abs(actual - expected) <= tolerance\n",
    "\n",
    "class TestTable1Proximity(unittest.TestCase):\n",
    "    \"\"\"\n",
    "    A TestCase class that compares key values in table_1\n",
    "    against expected (reference) values. Both table_1 and the\n",
    "    reference values are provided as parameters.\n",
    "    \"\"\"\n",
    "    def __init__(self, methodName='runTest', table_1=None, reference=None):\n",
    "        super(TestTable1Proximity, self).__init__(methodName)\n",
    "        self.table_1 = table_1\n",
    "        self.reference = reference\n",
    "\n",
    "    def test_proximity_aggregate_loss(self):\n",
    "        \"\"\"\n",
    "        Check that the 'Aggregate Loss' row in each column is within tolerance\n",
    "        of the expected reference value.\n",
    "        \"\"\"\n",
    "        row_label = 'Aggregate Loss'\n",
    "        for col, (expected_val, tolerance) in self.reference[row_label].items():\n",
    "            with self.subTest(column=col):\n",
    "                actual_str = str(self.table_1.loc[row_label, col])\n",
    "                actual_num = parse_value(actual_str)\n",
    "                condition = approximate_equal(actual_num, expected_val, tolerance)\n",
    "                self.assertTrue(\n",
    "                    condition,\n",
    "                    msg=(f\"Aggregate Loss mismatch for column '{col}': \"\n",
    "                         f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "                )\n",
    "                if condition:\n",
    "                    print(f\"Passed test to check tolerance of '{row_label}' in column '{col}': \"\n",
    "                          f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "\n",
    "    def test_number_of_banks(self):\n",
    "        \"\"\"\n",
    "        Check that the 'Number of Banks' row in each column is within tolerance\n",
    "        of the expected reference value.\n",
    "        \"\"\"\n",
    "        row_label = 'Number of Banks'\n",
    "        for col, (expected_val, tolerance) in self.reference[row_label].items():\n",
    "            with self.subTest(column=col):\n",
    "                actual_str = str(self.table_1.loc[row_label, col])\n",
    "                try:\n",
    "                    actual_num = float(actual_str)\n",
    "                except ValueError:\n",
    "                    actual_num = parse_value(actual_str)\n",
    "                condition = approximate_equal(actual_num, expected_val, tolerance)\n",
    "                self.assertTrue(\n",
    "                    condition,\n",
    "                    msg=(f\"Number of Banks mismatch for column '{col}': \"\n",
    "                         f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "                )\n",
    "                if condition:\n",
    "                    print(f\"Passed test to check tolerance of '{row_label}' in column '{col}': \"\n",
    "                          f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "\n",
    "    def test_share_rmbs(self):\n",
    "        \"\"\"\n",
    "        Check that the 'Share RMBS' row in each column is within tolerance\n",
    "        of the expected reference value.\n",
    "        \"\"\"\n",
    "        row_label = 'Share RMBS'\n",
    "        for col, (expected_val, tolerance) in self.reference[row_label].items():\n",
    "            with self.subTest(column=col):\n",
    "                actual_str = str(self.table_1.loc[row_label, col])\n",
    "                actual_num = parse_value(actual_str)\n",
    "                condition = approximate_equal(actual_num, expected_val, tolerance)\n",
    "                self.assertTrue(\n",
    "                    condition,\n",
    "                    msg=(f\"Share RMBS mismatch for column '{col}': \"\n",
    "                         f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "                )\n",
    "                if condition:\n",
    "                    print(f\"Passed test to check tolerance of '{row_label}' in column '{col}': \"\n",
    "                          f\"got {actual_num}, expected {expected_val} ± {tolerance}\")\n",
    "\n",
    "def load_tests(table_1, reference):\n",
    "    \"\"\"\n",
    "    Build and return a unittest.TestSuite using the provided table_1 DataFrame and\n",
    "    reference values.\n",
    "    \n",
    "    Args:\n",
    "        table_1 (pd.DataFrame): The DataFrame to test.\n",
    "        reference (dict): A dictionary of reference values.\n",
    "    \n",
    "    Returns:\n",
    "        unittest.TestSuite: The test suite.\n",
    "    \"\"\"\n",
    "    suite = unittest.TestSuite()\n",
    "    # Add each test by specifying the test method name and passing table_1 and reference\n",
    "    suite.addTest(TestTable1Proximity('test_proximity_aggregate_loss', table_1=table_1, reference=reference))\n",
    "    suite.addTest(TestTable1Proximity('test_number_of_banks', table_1=table_1, reference=reference))\n",
    "    suite.addTest(TestTable1Proximity('test_share_rmbs', table_1=table_1, reference=reference))\n",
    "    return suite\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # Define the reference dictionary with expected values and tolerances.\n",
    "    reference = {\n",
    "        'Aggregate Loss': {\n",
    "            'All Banks': (2.2e12, 0.3e12),\n",
    "            'Small Banks': (0.2e12, 0.1e12),\n",
    "            'Large Ex GSIB Banks': (1.1e12, 0.2e12),\n",
    "            'GSIB Banks': (1.0e12, 0.2e12)\n",
    "        },\n",
    "        'Number of Banks': {\n",
    "            'All Banks': (4844, 100),\n",
    "            'Small Banks': (4095, 100),\n",
    "            'Large Ex GSIB Banks': (732, 50),\n",
    "            'GSIB Banks': (17, 5)\n",
    "        },\n",
    "        'Share RMBS': {\n",
    "            'All Banks': (6.5, 2.0),\n",
    "            'Small Banks': (5.6, 2.0),\n",
    "            'Large Ex GSIB Banks': (10.8, 3.0),\n",
    "            'GSIB Banks': (17.9, 3.0)\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Build and run the test suite using the provided table_1 and reference values.\n",
    "    suite = load_tests(table_1, reference)\n",
    "    unittest.TextTestRunner().run(suite)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
